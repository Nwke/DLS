{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XgbimpoMZV5F"
   },
   "source": [
    "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=300, height=300></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h2 style=\"text-align: center;\"><b>Transfer Learning</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-nVLftxNZV5H"
   },
   "source": [
    "Как Вы уже знаете (если не знаете, рекомендуем посмотреть [5-ую](https://www.youtube.com/watch?v=cAJp2hh-_q8&t=4831s) и [6-ую](https://www.youtube.com/watch?v=RVk2RUW9Euk&t=40s) лекции нашего курса), в современных задачах обработки изображений, будь то задача обнаружения объектов, задача распознавания образов, задача (семантической) сегментации, задача классификации изображений и другие, всё чаще используют **свёрточные нейросети** (*Convolutional Neural Networks*, *CNN*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LA5ixc_nZV5H"
   },
   "source": [
    "Они показывают очень хорошие результаты, за ними стоит как [математический аппарат](https://stats.stackexchange.com/questions/269854/are-there-mathematical-reasons-for-convolution-in-neural-networks-beyond-expedie), так и эвристики, полученные опытным путём."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUH-Q7FuZV5I"
   },
   "source": [
    "В данном задании Вам предстоит познакомиться с архитектурами *AlexNet*, *VGG* и *Inception* и для каждой из этих моделей использовать технику **Transfer Learning**.  \n",
    "\n",
    "* **Transfer Learning** - это процесс дообучения на **новых данных** какой-либо нейросети, уже обученной до этого на других данных, обычно на каком-нибудь хорошем, большом (миллионы картинок) датасете (например, [ImageNet](http://www.image-net.org/) ~ 14 млн картинок)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\"><b>AlexNet</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet** - нейронная сеть, которая победила в ILSVRC (соревнование по классификации картинок из ImageNet) в 2012 году и стала основой для многих других архитектур. Впервые она была представлена в статье  “ImageNet Classification with Deep Convolutional Neural Networks”, над которой работал Джоффри Хинтон - человек, которого многие называют отцом современного computer vision.\n",
    "\n",
    "Архитектура описана на картинке ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet** состоит из 5 **сверточных** слоев, 3 **MaxPool** слоев и 2 **FullyConnected** слоев в конце. Обратите внимание, что в последнем пулинг слое окна, из которых берется максимум, пересекаются за счет того, что *stride*=2. Это изменение по сравнению с традиционным пулингом помогло снизить ошибку на 0.4%.\n",
    "\n",
    "По сути **AlexNet** это самая базовая архитектура для сверточной сети после LeNet, которую мы уже писали на предыдущем занятии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\"><b>VGG</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один **сверточный** слой с фильтром 5$\\times$5 можно заменить двумя подряд идущими слоями с фильтрами размером 3$\\times$3, так как **воспринимаемая область** картинки у них будет одинаковой. При этом уменьшиться количество параметров, поэтому такую сеть будет легче обучать. \n",
    "\n",
    "На момент создания VGG люди уже заметили, что чем больше слоев в нейросети, тем выше ее точность. Заменяя большие фильтры на несколько фильтров 3$\\times$3 исследователи получили глубокую нейросеть с меньшим количеством параметров. Архитектура VGG-16 (версии VGG с 16 слоями) представлена на картинке ниже:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*0Tk4JclhGOCR_uLe6RKvUQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда говорят **VGG**, то чаще всего имеют ввиду **VGG-16** или **VGG-19**. Более глубоких версий **VGG** нет, так как после 19 слоев точность начинает падать.\n",
    "\n",
    "Чтобы добиться высоких результатов в соревновании при обучении и валидации нейросети использовались дополнительные премы, подробнее о которых можно прочитать в [статье на Medium](https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"text-align: center;\"><b>Inception v1</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассмотрим идею, которая подтолкнула исследователей к созданию этой архитектуры.\n",
    "\n",
    "Площадь, которую занимает классифицируемый объект, может очень сильно отличаться. Пример на картинке ниже: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*aBdPBGAeta-_AM4aEyqeTQ.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для извлечения информации с большой площади лучше всего подходят **большие** фильтры, и наоборот для маленьких объектов лучше **маленькие** фильтры. \n",
    "* Глубокие нейронные сети намного сложнее обучать: в них появляется проблема **затухания градиента** и они **переобучаются**.\n",
    "Чтобы решить первую проблему исследователи придумали **Incepton** модуль, который применяет фильтры разного размера и затем склеивает полученные каналы. При этом извлекается как информация из больших объектов, так и из маленьких. Простейшая реализация модуля выглядит так:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*DKjGRDd_lJeUfVlY50ojOA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализацию можно сделать более эффективной, если сначала уменьшить количество каналов с помощью **сверточного слоя** 1$\\times$1 и лишь затем применить **слой** с фильтрами 5$\\times$5. Сокращение вычислений происходит за счет того, что мы сначала **уменьшаем размерность** данных и лишь затем преобразовываем их. Продвинутая реализация:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*U_McJnp7Fnif-lw9iIC5Bw.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть состоит из **корня** (нескольких сверточных слоев) и **Inception** модулей идущих за ним. Оранжевым прямоугольников выделен корень, а фиолетовыми - **вспомогательные классификаторы**. Именно они помогают бороться со второй проблемой, которую мы упомянули ранее. Наша функция потерь - взвешенная сумма **LogLoss** на двух **вспомогательных классификаторах** и **основном** в конце нейронной сети.\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*uW81y16b-ptBDV8SIT1beQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После Inception v1 были представлены 2, 3 и 4 версии, прочитать о которых вы можете  в [статье на Medium](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bx9ZdMmMZV5I"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Transfer Learning</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-EtKIoMZV5J"
   },
   "source": [
    "Теперь мы перейдем к тому, как можно использовать уже обученные нейросети, чтобы ускорить свою работу.\n",
    "\n",
    "Давайте вспомним общую архитектуру CNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWER66eyZV5K"
   },
   "source": [
    "<img src=\"https://www.kernix.com/doc/data/cnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bihUjSwGZV5L"
   },
   "source": [
    "С помощью операций *свёртки (convolution)* и *пулинга (pooling)* всё, что расположено до этапа *classification*, по сути **извлекает признаки из объектов, подающихся на вход** (картинок, в данном случае). То есть вместо того, чтобы самим пытаться как-то описать картинки для хорошей работы классификатора, мы предоставляем заняться этим нейросети (обучая её методом обратного распространения ошибки ([лекция 4](https://www.youtube.com/watch?v=HZDOhHAg5_g)))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gi9ad_suZV5L"
   },
   "source": [
    "**Вопрос (творческий):**  А какие признаки для картинок приходят Вам в голову? (считать, что картинки цветные (3 канала), все одинакового размера)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AMX8k3uZV5M"
   },
   "source": [
    "**Ответ:** <Ваши мысли>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpWxCG1_ZV5N"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Описание метода</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5JCuydFZV5O"
   },
   "source": [
    "Представим теперь, что eсть свой набор данных, и Вы хотите научить сеть классифицировать объекты из Вашей выборки.  \n",
    "Есть 4 возможных подхода к задаче:\n",
    "\n",
    "* **1. Написать свою собственную нейронную сеть**\n",
    "    * Если Вас зовут не Ian Godfellow, Geoffry Hinton или Andrew Ng, то не рекомендуется пользоваться этим способом\n",
    "    * May the force be with You\n",
    "\n",
    "* **2. CNN как средство для извлечения признаков (Feature Extractor)** \n",
    "    * Берём сетку, обученную на ImageNet \n",
    "    * Убираем последние Fully-Connected слои сети, отвечающие за классификацию. Веса предыдущих слоёв **заморожены**, мы их не трогаем. Теперь сеть выдаёт не метки классов, а то, что поступало на вход Fully-Connected (развёрнутый в строку \"параллелепипед\" HxWxNUM_FILTERS с последнего слоя перед FC)\n",
    "    * Запускаем сеть на новом датасете, получаем выходы сети для всех объектов - это и есть их признаки, полученные сетью\n",
    "    * Обучаем на этих признаках какой-либо классификатор (свою Fully-Connected сеть, например)\n",
    "    * Теперь у нас есть сеть, работающая хорошо на нашем датасете\n",
    "\n",
    "* **3. CNN, которую можно дообучить (Fine Tuning)**  \n",
    "    * Берём сетку, обученную на ImageNet  \n",
    "    * Убираем последние Fully-Connected слои сети, отвечающие за классификацию.  \n",
    "    * Теперь всё же *распространяем backpropagation ещё на сколько-то слоёв назад (размораживаем веса в этих слоях)*, чтобы скорректировать их под новые данные. Можно распространить обучение и на всю сеть, но часто первые слои всё же замораживают, поскольку они (как ожидается) извлекают более общие признаки. А ещё обучать всю сеть всё же дольше, чем несколько слоёв. Всё зависит от того, какого качества Вы хотите добиться\n",
    "    * Теперь сеть выдаёт не метки классов, а то, что поступало на вход Fully-Connected (веса последних (или всех) слоёв были изменены под наши данные)\n",
    "    * Обучаем на этих признаках какой-либо классификатор (свою Fully-Connected сеть, например)\n",
    "    * Теперь у нас есть сеть, работающая хорошо на нашем датасете\n",
    "\n",
    "* **4. Использовать предобученную модель \"из коробки\"**  \n",
    "    * То есть взять уже готовую нейронную сеть и использовать её (её параметры (W, b..), ведь сеть характеризуется параметрами, если архитектура известна) для решения своей задачи. Например, [здесь](https://github.com/BVLC/caffe/wiki/Model-Zoo) люди часто выкладывают веса моделей, обученных для решения их специфических задач.  \n",
    "\n",
    "\n",
    "В зависимости от количества и природы Ваших данных есть выбор из **нескольких стратегий Transfer Learning**, а именно:\n",
    "\n",
    "* *У Вас **мало данных** ($\\le$ 10k), и они **похожи** на данные, на которых была обучена сеть до этого*  \n",
    "Если данные совсем похожи, можно попробовать использовать готовую модель. Если качество Вас не устраивает, то тогда стоит использовать CNN для извлечения признаков и обучить свой классификатор на этих данных (2-ой способ выше). Так как данные похожи на те, на которых обучалась сеть, то высокоуровневые признаки, полученные с помощью последних слоёв сети, должны оказаться информативными. Если делать в этом случае Fine-Tuning (3 способ), то сеть может переобучиться, поскольку данных мало.\n",
    "* *У Вас **мало данных** ($\\le$ 10k), и они **не похожи** на данные, на которых была обучена сеть до этого*  \n",
    "Самый невыгодный случай. Здесь мы не можем ожидать от сети, что выходы последних слоёв будут информативными для новых данных. Следует также действовать в соответствие со 2-ым способом, но брать как признаки выходы более ранних слоёв, ведь, как мы помним, они (как ожидается) соответствуют более общим паттернам в данных.\n",
    "* *У Вас **много данных** ($\\ge$ 10k), и они **похожи** на данные, на которых была обучена сеть до этого*  \n",
    "В этом случае можем смело делать Fine-Tuning (3 способ) (если не устроило качество модели \"из коробки\"), ведь данных много, и вероятность переобучения меньше. В данном случае имеет смысл попробовать разморохить веса последних нескольких слоёв (зависит от того, сколько у Вас времени и вычислительной мощности, можно разморозить и всю сеть)\n",
    "* *У Вас **много данных** ($\\ge$ 10k), и они **не похожи** на данные, на которых была обучена сеть до этого*\n",
    "В принципе, подход тот же, что и в случае похожих данных, то есть мы файнтюним практически всю нейросеть. Однако мы вольны в этом случае полнстью менять все параметры (и гиперпараметры) нейросети, ведь по сути мы пользуемся только её архитектурой, забывая о том, что она уже была когда-то обучена. Но часто веса предобученной сети оставляют в качестве инициализации для обучения на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gEoe0qi_ZV5O"
   },
   "source": [
    "Надеемся, что теперь Вам стало понятнее, как обучать крутые сети на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CU8jKks8ZV5P"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Переходим к практике</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyvXMfhZZV5Q"
   },
   "source": [
    "<p style=\"text-align: center;\"><i>(основано на http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XFlfXQsZV5R"
   },
   "source": [
    "Мы будем пользоваться библиотекой PyTorch. Если Вы её ещё не установили, то вот [инструкция на Wiki по установке PyTorch](https://github.com/deepmipt/dlschl/wiki/%D0%98%D0%BD%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE-%D1%83%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B5-PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xF5zpbFgkIp"
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2sTUHmNZV5S"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "weT3x9DQZV5X"
   },
   "source": [
    "### В чём состоит задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8paFz_5ZV5Y"
   },
   "source": [
    "Вам предстоит попробовать использовать  типа архитектур свёрточных нейросетей - **AlexNet (сделано за Вас в примере), VGG16, Inception_v3** - как *Feature Extractor*, с помощью *Fine Tuning* и *\"из коробки\"*. \n",
    "\n",
    "**Для каждого пункта нужно:**\n",
    "- вывести график loss'а на обучающей и на валидационной выборке\n",
    "- вывести качество модели (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99LAkQNVZV5Y"
   },
   "source": [
    "### Данные  \n",
    "\n",
    "В данном задании используются сети (из библиотеки **torchvision**), предобученные на датасете ImageNet.  \n",
    "В качестве новых данных будет датасет Меравьи vs Пчёлы, Вам нужно скачать его отсюда: **[Муравьи vs Пчёлы](https://download.pytorch.org/tutorial/hymenoptera_data.zip)**, *являющийся частью датасета ImageNet*. В нём 400 картинок, ~250 обучение и ~150 валидация (тест)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8gGQA6CbZV5Z"
   },
   "source": [
    "### Функции для отрисовки и обучения модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnoL3STNZV5a"
   },
   "source": [
    "* Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lKZZSUYZV5b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './hymenoptera_data\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a8876968457c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0;32m     20\u001b[0m                                           data_transforms[x])\n\u001b[1;32m---> 21\u001b[1;33m                   for x in ['train', 'val']}\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m# специальный класс для загрузки данных в виде батчей\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
      "\u001b[1;32m<ipython-input-2-a8876968457c>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0;32m     20\u001b[0m                                           data_transforms[x])\n\u001b[1;32m---> 21\u001b[1;33m                   for x in ['train', 'val']}\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m# специальный класс для загрузки данных в виде батчей\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[0;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[0;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                                           target_transform=target_transform)\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(dir)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './hymenoptera_data\\\\train'"
     ]
    }
   ],
   "source": [
    "# Преобразование обучающих данных для расширения обучающей выборки и её нормализация\n",
    "# Для валидационной (тестовой) выборки только нормализация\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(244),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(244),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "# папка с данными\n",
    "data_dir = './hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "# специальный класс для загрузки данных в виде батчей\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                              shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03FqarW9ZV5e"
   },
   "source": [
    "Размеры обучающей и валидационной выборок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxjRd-5OZV5f",
    "outputId": "e5e412ad-1fe6-4cb9-817a-a1637a27042b"
   },
   "outputs": [],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vx3QiNAcZV5k"
   },
   "source": [
    "**Вопрос (на понимание кода выше):**  \n",
    "1. В DataLoader() выше стоит \"shuffle=True\". Для чего это нужно?\n",
    "2. Сколько картинок будет в каждом батче?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZPe4pfoZV5l"
   },
   "source": [
    "**Ответ:** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXqEVA1ZZV5m"
   },
   "source": [
    "* Посмотрим на картинки из датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQG2_RrvZV5n",
    "outputId": "7f67e611-fcfb-49e3-b465-421764ec7a6a"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "\n",
    "# Получим 1 батч (картнки-метки) из обучающей выборки\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Расположим картинки рядом\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqOOeBFpZV6T"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3fefed565ee6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "for i in dataloaders['train']:\n",
    "    print(i[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UzP9AdAwZV5s"
   },
   "source": [
    "Следующая функция будет использоваться для обучения модели. Аргументы:  \n",
    "* model $-$ нейросеть\n",
    "* loss $-$ оптимизируемая функция (criterion, cost function, objective)\n",
    "* optimizer $-$ оптимизационный алгоритм\n",
    "* scheduler $-$ политика изменения learning_rate\n",
    "* num_epochs $-$ количество итераций обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6kaT01UZV5u"
   },
   "source": [
    "**Задание**: Вам нужно модифицировать эту функцию, чтобы она возвращала ещё и массивы loss'а на обучающей и валидационной выборках (чтобы потом Вы могли нарисовать графики). Можете модифицировать эту функцию как угодно, лишь бы она правильно работала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yGD0lrIaZV5u"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    #Ваш код здесь\n",
    "    losses = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # каждя эпоха имеет обучающую и тестовую стадии\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # установаить модель в режим обучения\n",
    "            else:\n",
    "                model.train(False)  # установить модель в режим предсказания\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # итерируемся по батчам\n",
    "            for data in dataloaders[phase]:\n",
    "                # получаем картинки и метки\n",
    "                inputs, labels = data\n",
    "\n",
    "                # оборачиваем в переменные\n",
    "                if use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                else:\n",
    "                    inputs, labels = inputs, labels\n",
    "\n",
    "                # инициализируем градиенты параметров\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward pass + оптимизируем только если это стадия обучения\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # статистика\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += int(torch.sum(preds == labels.data))\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            \n",
    "            # Ваш код здесь\n",
    "            losses[phase].append(epoch_loss)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # если достиглось лучшее качество, то запомним веса модели\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # загрузим лучшие веса модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VsQHae5ZV5x"
   },
   "source": [
    "* Функция для отрисовки тестовых изображений и предсказаний для них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Pyth5M_FZV5y"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, data in enumerate(dataloaders['val']):\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Функция для измерения точности модели на валидационном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.train(False)\n",
    "    \n",
    "    runninig_correct = 0\n",
    "    for data in dataloaders['val']:\n",
    "        # получаем картинки и метки\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # переносим на gpu, если возможно\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        # forard pass\n",
    "        output = model(inputs)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        runninig_correct += int(torch.sum(predicted == labels))\n",
    "        \n",
    "    return runninig_correct / dataset_sizes['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Afrq3SA1ZV53"
   },
   "source": [
    "### Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNCYHC8oZV53"
   },
   "source": [
    "Для каждой из следующих нейросетей:\n",
    "* **AlexNet** (уже сделано в примере)\n",
    "* **VGG16**\n",
    "* **Inception_v3**\n",
    "\n",
    "Напишите код и выведите результат (график лосса, accuracy и вывод примера классификации картинок с визализацией (с помощью функции `vizualize_model()`)) для трёх способов:\n",
    "* Использование готовой нейросети **\"из коробки\"**\n",
    "* Использование нейросети как **Feature Extractor**\n",
    "* **Fine Tuning** нейросети\n",
    "\n",
    "Для каждого пункта нужно:\n",
    "* сделать с сетью то, что нужно в пункте (\"из коробки\", FE или FT)\n",
    "* вывести график loss'а на обучающей и на валидационной выборке\n",
    "* вывести качество модели (accuracy) на валидационной (тестовой) выборке\n",
    "* (по желанию) использовать функцию visualize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XzWmgE7ZV54"
   },
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6zW24cSZV56"
   },
   "source": [
    "*ПРИМЕЧАНИЕ: Здесь не выведены графики loss'а и не использована visualize_model(). От Вас это ожидается.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmLx0a_uZV56"
   },
   "source": [
    "Загрузка модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1mHbrvXAZV57"
   },
   "outputs": [],
   "source": [
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_g9XqLsZV5-"
   },
   "source": [
    "Посмотрим, что внутри:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-OLqslsZV6A",
    "outputId": "ee24b182-f8bf-4c20-cdcd-14959790b70d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9LYMESWZV6E"
   },
   "source": [
    "Видим, что на вход классификатору (classifier) подаётся *9216 признаков*. Это и будет размер входа для нашего нового классификатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FA5qB523ZV6G"
   },
   "source": [
    "* **Fine Tuning** способ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxucpA3FZV6H"
   },
   "source": [
    "Сконфигурируем - изменим FC-слой и зададим *cost function* и *оптимизирующий алгоритм*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JKPZYgVZV6J"
   },
   "source": [
    "(*по умолчанию backpropagation распространяется на все слои, поэтому здесь мы только заменяем FC-слой на свой классификатор*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ppIGmkz4ZV6K"
   },
   "outputs": [],
   "source": [
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 9216\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model.classifier = nn.Linear(num_features, 2)\n",
    "\n",
    "# Использовать ли GPU\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "# В качестве cost function используем кросс-энтропию\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# В качестве оптимизатора - стохастический градиентный спуск\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Умножает learning_rate на 0.1 каждые 7 эпох (это одна из эвристик, не было на лекциях)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXDZK7JVZV6X",
    "outputId": "3fc3653a-4d28-4b59-f615-563a3691fb9b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.2380 Acc: 0.5984\n",
      "val Loss: 0.1679 Acc: 0.5921\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.1671 Acc: 0.6434\n",
      "val Loss: 0.1750 Acc: 0.6053\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.1674 Acc: 0.5451\n",
      "val Loss: 0.1738 Acc: 0.6316\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.1657 Acc: 0.5984\n",
      "val Loss: 0.1616 Acc: 0.6645\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1630 Acc: 0.6352\n",
      "val Loss: 0.1590 Acc: 0.7039\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1555 Acc: 0.6393\n",
      "val Loss: 0.1956 Acc: 0.6513\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1581 Acc: 0.6393\n",
      "val Loss: 0.1515 Acc: 0.7368\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1348 Acc: 0.7172\n",
      "val Loss: 0.1403 Acc: 0.7237\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1385 Acc: 0.6926\n",
      "val Loss: 0.1407 Acc: 0.7171\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1360 Acc: 0.7172\n",
      "val Loss: 0.1414 Acc: 0.7237\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1315 Acc: 0.7090\n",
      "val Loss: 0.1438 Acc: 0.7368\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1311 Acc: 0.6885\n",
      "val Loss: 0.1380 Acc: 0.7171\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1209 Acc: 0.7131\n",
      "val Loss: 0.1361 Acc: 0.7039\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1192 Acc: 0.7664\n",
      "val Loss: 0.1361 Acc: 0.7105\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1139 Acc: 0.7582\n",
      "val Loss: 0.1367 Acc: 0.6974\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1172 Acc: 0.7295\n",
      "val Loss: 0.1384 Acc: 0.7105\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1111 Acc: 0.7705\n",
      "val Loss: 0.1392 Acc: 0.7105\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1214 Acc: 0.7623\n",
      "val Loss: 0.1403 Acc: 0.7105\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1258 Acc: 0.7377\n",
      "val Loss: 0.1420 Acc: 0.7171\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.7541\n",
      "val Loss: 0.1420 Acc: 0.6974\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1150 Acc: 0.7787\n",
      "val Loss: 0.1416 Acc: 0.7039\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1144 Acc: 0.7582\n",
      "val Loss: 0.1415 Acc: 0.7039\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1106 Acc: 0.8156\n",
      "val Loss: 0.1415 Acc: 0.7039\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1087 Acc: 0.7787\n",
      "val Loss: 0.1415 Acc: 0.7039\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1113 Acc: 0.7582\n",
      "val Loss: 0.1414 Acc: 0.7039\n",
      "\n",
      "Training complete in 1m 9s\n",
      "Best val Acc: 0.736842\n"
     ]
    }
   ],
   "source": [
    "model, losses = train_model(model, loss_fn, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98c84d93c8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8CullI/R4FdKKR+jwa+UUj7m/wPSd3SJXm7W1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Построим график лосса при обучении и валидации\n",
    "\n",
    "#Ваш код здесь\n",
    "plt.plot(losses['train'])\n",
    "plt.plot(losses['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'AlexNet_fine_tune.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('AlexNet_fine_tune.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-040f9e5cec62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: {0:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(evaluate(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wy3MHj1ZV6d"
   },
   "source": [
    "Видим, что Fine Tuning AlexNet'а не дал хороших результатов, поскольку качество получается ~70%, что маловато для свёрточной нейросети для задачи классификации картинок. Посмотрим, как будет с AlexNet в роли Feature Extractor'а."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vyM6SLNZZV6g"
   },
   "source": [
    "* **Feature Extractor** способ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "81XVFTJiZV6h"
   },
   "outputs": [],
   "source": [
    "model_extractor = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XUzgLSlZV6m"
   },
   "source": [
    "Помним, что по-умолчанию все слои нейросети обучаются заново:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8kiGUDNqZV6n",
    "outputId": "c98ee36f-0ba0-40dd-886a-3b794c438850"
   },
   "outputs": [],
   "source": [
    "for param in model_extractor.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOTXa28JZV6s"
   },
   "source": [
    "Сделаем так, чтобы на них *не распространялся backpropagation* (заморозим их), и подменим классификатор (ведь старый уже с весами для ImageNet'а)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "K7rW4vKaZV6u"
   },
   "outputs": [],
   "source": [
    "# замораживаем параметры (веса)\n",
    "for param in model_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 9216\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_extractor.classifier = nn.Linear(num_features, 2)\n",
    "\n",
    "# Использовать ли GPU\n",
    "if use_gpu:\n",
    "    model_extractor = model_extractor.cuda()\n",
    "\n",
    "# В качестве cost function используем кросс-энтропию\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучаем только классификатор\n",
    "optimizer = optim.SGD(model_extractor.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Умножает learning_rate на 0.1 каждые 7 эпох (это одна из эвристик, не было на лекциях)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVL7TjRIZV6z",
    "outputId": "e26832fb-26f2-4c69-ae28-f55cfd481524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.7663 Acc: 0.7459\n",
      "val Loss: 0.9503 Acc: 0.8289\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.0948 Acc: 0.8074\n",
      "val Loss: 1.5360 Acc: 0.8421\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.4391 Acc: 0.8033\n",
      "val Loss: 1.2560 Acc: 0.8750\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.0135 Acc: 0.8484\n",
      "val Loss: 1.5536 Acc: 0.8553\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.7094 Acc: 0.8934\n",
      "val Loss: 1.2940 Acc: 0.9013\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.8080 Acc: 0.8852\n",
      "val Loss: 1.5027 Acc: 0.8816\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.9291 Acc: 0.8525\n",
      "val Loss: 1.7495 Acc: 0.8487\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.5847 Acc: 0.9139\n",
      "val Loss: 1.0808 Acc: 0.8947\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.4932 Acc: 0.9057\n",
      "val Loss: 1.1085 Acc: 0.8750\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.6181 Acc: 0.9262\n",
      "val Loss: 1.0771 Acc: 0.8750\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.4672 Acc: 0.9221\n",
      "val Loss: 1.0184 Acc: 0.8816\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.4062 Acc: 0.9180\n",
      "val Loss: 1.0021 Acc: 0.8750\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.2990 Acc: 0.9303\n",
      "val Loss: 1.0152 Acc: 0.8750\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.4215 Acc: 0.9180\n",
      "val Loss: 1.0336 Acc: 0.8750\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.2571 Acc: 0.9385\n",
      "val Loss: 1.0345 Acc: 0.8816\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.4646 Acc: 0.9221\n",
      "val Loss: 1.0352 Acc: 0.8882\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.2007 Acc: 0.9262\n",
      "val Loss: 1.0356 Acc: 0.8882\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.2690 Acc: 0.9262\n",
      "val Loss: 1.0345 Acc: 0.8882\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.2331 Acc: 0.9385\n",
      "val Loss: 1.0344 Acc: 0.8816\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.4262 Acc: 0.9303\n",
      "val Loss: 1.0351 Acc: 0.8882\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9549\n",
      "val Loss: 1.0331 Acc: 0.8882\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.3063 Acc: 0.9426\n",
      "val Loss: 1.0335 Acc: 0.8882\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.3651 Acc: 0.9098\n",
      "val Loss: 1.0336 Acc: 0.8882\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.4022 Acc: 0.9303\n",
      "val Loss: 1.0337 Acc: 0.8882\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1954 Acc: 0.9344\n",
      "val Loss: 1.0336 Acc: 0.8882\n",
      "\n",
      "Training complete in 0m 48s\n",
      "Best val Acc: 0.901316\n",
      "CPU times: user 17.7 s, sys: 14.6 s, total: 32.2 s\n",
      "Wall time: 47.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_extractor,losses = train_model(model_extractor, loss_fn, optimizer, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_extractor.state_dict(), 'AlexNet_extractor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_extractor.load_state_dict(torch.load('AlexNet_extractor.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(evaluate(model_extractor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5Tn3EAXZV66"
   },
   "source": [
    "Видим, что качество намного лучше: ~90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Смешанный** способ:\n",
    "Мы будем обучать не только последний **fully connected** слой, но и несколько предпоследних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_mixed = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_to_unfreeze = 5\n",
    "\n",
    "# Выключаем подсчет градиентов для слоев, которые не будем обучать\n",
    "for param in model_mixed.features[:-layers_to_unfreeze].parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 9216\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_mixed.classifier = nn.Linear(num_features, 2)\n",
    "\n",
    "# Использовать ли GPU\n",
    "if use_gpu:\n",
    "    model_mixed = model_mixed.cuda()\n",
    "\n",
    "# В качестве cost function используем кросс-энтропию\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучаем последние layers_to_unfreeze слоев из сверточной части и fully connected слой \n",
    "# parameters() возвращает просто список тензоров парамтеров, поэтому два таких списка можно сложить\n",
    "optimizer = optim.SGD(list(model_mixed.features.parameters())[-layers_to_unfreeze:] + \n",
    "                      list(model_mixed.classifier.parameters()), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Умножает learning_rate на 0.1 каждые 7 эпох (это одна из эвристик, не было на лекциях)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.3147 Acc: 0.7910\n",
      "val Loss: 0.1504 Acc: 0.8092\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.1086 Acc: 0.8238\n",
      "val Loss: 0.1366 Acc: 0.8092\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.1184 Acc: 0.8033\n",
      "val Loss: 0.1443 Acc: 0.7697\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.1120 Acc: 0.7910\n",
      "val Loss: 0.1139 Acc: 0.8421\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.0805 Acc: 0.8730\n",
      "val Loss: 0.1836 Acc: 0.8026\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1014 Acc: 0.8197\n",
      "val Loss: 0.2734 Acc: 0.7961\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.0657 Acc: 0.8934\n",
      "val Loss: 0.1255 Acc: 0.9079\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.0720 Acc: 0.9139\n",
      "val Loss: 0.1241 Acc: 0.9013\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0500 Acc: 0.9180\n",
      "val Loss: 0.1140 Acc: 0.9013\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0471 Acc: 0.9098\n",
      "val Loss: 0.1140 Acc: 0.9013\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0403 Acc: 0.9139\n",
      "val Loss: 0.1193 Acc: 0.9013\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0458 Acc: 0.9139\n",
      "val Loss: 0.1210 Acc: 0.9013\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0441 Acc: 0.9303\n",
      "val Loss: 0.1182 Acc: 0.9079\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0374 Acc: 0.9385\n",
      "val Loss: 0.1216 Acc: 0.9145\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0505 Acc: 0.9221\n",
      "val Loss: 0.1214 Acc: 0.9145\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0370 Acc: 0.9303\n",
      "val Loss: 0.1217 Acc: 0.9079\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0389 Acc: 0.9344\n",
      "val Loss: 0.1217 Acc: 0.9145\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0345 Acc: 0.9344\n",
      "val Loss: 0.1218 Acc: 0.9145\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0409 Acc: 0.9180\n",
      "val Loss: 0.1219 Acc: 0.9211\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0381 Acc: 0.9221\n",
      "val Loss: 0.1222 Acc: 0.9211\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0380 Acc: 0.9344\n",
      "val Loss: 0.1222 Acc: 0.9145\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0410 Acc: 0.9262\n",
      "val Loss: 0.1222 Acc: 0.9145\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0357 Acc: 0.9426\n",
      "val Loss: 0.1222 Acc: 0.9145\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0426 Acc: 0.9057\n",
      "val Loss: 0.1222 Acc: 0.9145\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0390 Acc: 0.9303\n",
      "val Loss: 0.1222 Acc: 0.9145\n",
      "\n",
      "Training complete in 0m 53s\n",
      "Best val Acc: 0.921053\n",
      "CPU times: user 25.2 s, sys: 17.2 s, total: 42.4 s\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_, losses = train_model(model_mixed, loss_fn, optimizer, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_mixed.state_dict(), 'AlexNet_mixed.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_mixed.load_state_dict(torch.load('AlexNet_mixed.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(evaluate(model_mixed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ue3eslBDZV67"
   },
   "source": [
    "**Вопрос 1 (важный):** С чем связано повышение качества если мы перестаем учить всю сеть? (Подсказка: посмотрите на датасет и на то, как он согласуется с 4-мя ситуациями, описанными выше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zz5G8Z2IZV69"
   },
   "source": [
    "**Ответ (важный):** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 2**: Почему разморозка последних слоев не дает прироста к точности, хотя разморозить несколько послдних слоев обычно хорошеее решение для классификации похожего датасета? (Вопрос на внимательность)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ответ:** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90JYIt39ZV7W"
   },
   "source": [
    "### Другие, более современные нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6McVYGoIZV7X"
   },
   "source": [
    "**Вопрос:** Какую стратегию Вы выберете, учитывая размер и специфику нового датасета?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gg9c_ngBZV7X"
   },
   "source": [
    "**Ответ:** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLb_VQ-nZV7Y"
   },
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HrmyWuu-ZV7Z"
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "model_extractor = models.vgg16(pretrained=True)\n",
    "\n",
    "# замораживаем параметры (веса)\n",
    "for param in model_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 25088\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_extractor.classifier = nn.Linear(num_features, 2)\n",
    "\n",
    "# Использовать ли GPU\n",
    "if use_gpu:\n",
    "    model_extractor = model_extractor.cuda()\n",
    "\n",
    "# В качестве cost function используем кросс-энтропию\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучаем только классификатор\n",
    "optimizer = optim.SGD(model_extractor.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Умножает learning_rate на 0.1 каждые 7 эпох (это одна из эвристик, не было на лекциях)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5138 Acc: 0.8279\n",
      "val Loss: 0.7815 Acc: 0.8092\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2969 Acc: 0.8852\n",
      "val Loss: 0.4459 Acc: 0.9211\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2497 Acc: 0.9139\n",
      "val Loss: 0.3575 Acc: 0.9408\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.1842 Acc: 0.9303\n",
      "val Loss: 0.5043 Acc: 0.9211\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1930 Acc: 0.9385\n",
      "val Loss: 0.5391 Acc: 0.9276\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.3454 Acc: 0.9139\n",
      "val Loss: 0.5791 Acc: 0.9276\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.4228 Acc: 0.9057\n",
      "val Loss: 0.5413 Acc: 0.9408\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1249 Acc: 0.9467\n",
      "val Loss: 0.5216 Acc: 0.9342\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0907 Acc: 0.9467\n",
      "val Loss: 0.5056 Acc: 0.9342\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.2949 Acc: 0.9303\n",
      "val Loss: 0.4889 Acc: 0.9408\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9754\n",
      "val Loss: 0.4942 Acc: 0.9276\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.2037 Acc: 0.9426\n",
      "val Loss: 0.4826 Acc: 0.9342\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9672\n",
      "val Loss: 0.4756 Acc: 0.9474\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1820 Acc: 0.9549\n",
      "val Loss: 0.4764 Acc: 0.9474\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1895 Acc: 0.9549\n",
      "val Loss: 0.4756 Acc: 0.9408\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1064 Acc: 0.9426\n",
      "val Loss: 0.4756 Acc: 0.9408\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1588 Acc: 0.9508\n",
      "val Loss: 0.4752 Acc: 0.9408\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1387 Acc: 0.9467\n",
      "val Loss: 0.4744 Acc: 0.9408\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.2088 Acc: 0.9221\n",
      "val Loss: 0.4735 Acc: 0.9342\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0936 Acc: 0.9467\n",
      "val Loss: 0.4730 Acc: 0.9342\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0957 Acc: 0.9590\n",
      "val Loss: 0.4734 Acc: 0.9408\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1666 Acc: 0.9467\n",
      "val Loss: 0.4734 Acc: 0.9408\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9713\n",
      "val Loss: 0.4735 Acc: 0.9408\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1248 Acc: 0.9467\n",
      "val Loss: 0.4734 Acc: 0.9408\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0340 Acc: 0.9754\n",
      "val Loss: 0.4735 Acc: 0.9408\n",
      "\n",
      "Training complete in 5m 40s\n",
      "Best val Acc: 0.947368\n"
     ]
    }
   ],
   "source": [
    "# Запустите обучение\n",
    "\n",
    "model_extractor, losses = train_model(model_extractor, loss_fn, optimizer, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_extractor.state_dict(), 'VGG16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_extractor.load_state_dict(torch.load('VGG16.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(evaluate(model_extractor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lw96OH0SZV7b"
   },
   "source": [
    "### Inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Нужно поменять размер картинок на 299, иначе будет ошибка, так как размерность станет отрицательной.\n",
    "# Это вызвано тем, что нейросеть изначально обучалась на картинках размера 299.\n",
    "\n",
    "# Результирующий размер картинок определяется трансформациями\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(299),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Сам объект датасета\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# специальный класс для загрузки данных в виде батчей\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                              shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HrmyWuu-ZV7Z"
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "model_extractor = models.inception_v3(pretrained=True)\n",
    "\n",
    "# замораживаем параметры (веса)\n",
    "for param in model_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 2048\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_extractor.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Использовать ли GPU\n",
    "if use_gpu:\n",
    "    model_extractor = model_extractor.cuda()\n",
    "\n",
    "# В качестве cost function используем кросс-энтропию\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучаем только классификатор\n",
    "optimizer = optim.SGD(model_extractor.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Умножает learning_rate на 0.1 каждые 7 эпох (это одна из эвристик, не было на лекциях)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.1552 Acc: 0.6926\n",
      "val Loss: 0.1010 Acc: 0.8750\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.1563 Acc: 0.6557\n",
      "val Loss: 0.0868 Acc: 0.8750\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.1104 Acc: 0.7787\n",
      "val Loss: 0.0623 Acc: 0.9211\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.7500\n",
      "val Loss: 0.0793 Acc: 0.8618\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1180 Acc: 0.7828\n",
      "val Loss: 0.0920 Acc: 0.8553\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1172 Acc: 0.7500\n",
      "val Loss: 0.0568 Acc: 0.9079\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1109 Acc: 0.8156\n",
      "val Loss: 0.0538 Acc: 0.9079\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.0979 Acc: 0.8033\n",
      "val Loss: 0.0619 Acc: 0.9013\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0893 Acc: 0.8484\n",
      "val Loss: 0.0580 Acc: 0.8882\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1012 Acc: 0.8197\n",
      "val Loss: 0.0547 Acc: 0.9276\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.7787\n",
      "val Loss: 0.0541 Acc: 0.9079\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.8566\n",
      "val Loss: 0.0580 Acc: 0.9145\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0814 Acc: 0.8320\n",
      "val Loss: 0.0540 Acc: 0.9145\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0853 Acc: 0.8443\n",
      "val Loss: 0.0582 Acc: 0.9079\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0937 Acc: 0.8648\n",
      "val Loss: 0.0557 Acc: 0.9013\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0916 Acc: 0.8402\n",
      "val Loss: 0.0532 Acc: 0.9079\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1019 Acc: 0.8156\n",
      "val Loss: 0.0510 Acc: 0.9145\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0936 Acc: 0.8361\n",
      "val Loss: 0.0554 Acc: 0.8947\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0985 Acc: 0.8279\n",
      "val Loss: 0.0546 Acc: 0.9079\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0898 Acc: 0.8484\n",
      "val Loss: 0.0537 Acc: 0.8947\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0923 Acc: 0.8320\n",
      "val Loss: 0.0584 Acc: 0.9145\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1083 Acc: 0.8033\n",
      "val Loss: 0.0545 Acc: 0.9211\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0949 Acc: 0.8197\n",
      "val Loss: 0.0556 Acc: 0.9211\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1080 Acc: 0.7787\n",
      "val Loss: 0.0523 Acc: 0.9079\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1106 Acc: 0.7787\n",
      "val Loss: 0.0516 Acc: 0.9145\n",
      "\n",
      "Training complete in 3m 44s\n",
      "Best val Acc: 0.927632\n",
      "CPU times: user 2min 31s, sys: 1min 2s, total: 3min 34s\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_extractor.aux_logits = False\n",
    "model_extractor, losses = train_model(model_extractor, loss_fn, optimizer, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_extractor.state_dict(), 'Inceptionv3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_extractor.load_state_dict(torch.load('Inceptionv3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:.4f}\".format(evaluate(model_extractor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DK1TsMVZZV7h"
   },
   "source": [
    "**Вопрос:** Какая из сетей оказалась наилучшей? Как думаете, почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRukNGePZV7j"
   },
   "source": [
    "**Ответ:** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c57p6EIi6qv2"
   },
   "source": [
    "\n",
    "<h2 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CvOZffV6qv2"
   },
   "source": [
    "1). *cs231n: http://cs231n.github.io/transfer-learning/*\n",
    "\n",
    "2). *Туториал на PyTorch Tutorials: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial*\n",
    "\n",
    "3). *Статья на Medium про TL в PyTorch: https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4*  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "99LAkQNVZV5Y"
   ],
   "name": "[cnn]transfer_learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
